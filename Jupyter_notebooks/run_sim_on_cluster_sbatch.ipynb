{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "randomly choose seed - 14343\n",
      "--------------\n",
      "-----------------------------------------\n",
      "\"random_seed\" - 14343\n",
      "\"morphology_description\" - basal_oblique_tuft\n",
      "\"gmax_NMDA_to_AMPA_ratio\" - 1.000\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "spatial_clusters_matrix = \n",
      "-----------------------------------------\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 1  1  1 ...  1  1  1]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [55  4 59 ... 41 41 41]\n",
      " [27 48  4 ... 43 43 43]\n",
      " [41  9  9 ... 47 47 47]]\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "segments_to_keep = \n",
      "-----------------------------------------\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638]\n",
      "-----------------------------------------\n",
      "845.34678 \n",
      "845.34678 \n",
      "845.34678 \n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------\n",
      "segments_to_drop:\n",
      "-----------------\n",
      "(0,)\n",
      "[]\n",
      "-----------------\n",
      "-------------------------------------\\\n",
      "temperature is 6.30 degrees celsius\n",
      "dt is 0.0250 ms\n",
      "-------------------------------------/\n",
      "...\n",
      "------------------------------------------------------------------------------\\\n",
      "going to insert excitatory spikes per 100ms in range: [17, 140]\n",
      "going to insert inhibitory spikes per 100ms in range: [0, 268]\n",
      "preparing for single simulation took 1.2255 seconds\n",
      "single simulation took 0.10 minutes\n",
      "simulation with no output spikes. tossing a coin...\n",
      "decided to keep.\n",
      "\n",
      "\n",
      "data collection per single simulation took 0.0839 seconds\n",
      "-----------------------------------------------------------\n",
      "finished simulation 1: num output spikes = 0\n",
      "entire simulation took 0.12 minutes\n",
      "------------------------------------------------------------------------------/\n",
      "...\n",
      "------------------------------------------------------------------------------\\\n",
      "going to insert excitatory spikes per 100ms in range: [8, 133]\n",
      "going to insert inhibitory spikes per 100ms in range: [0, 208]\n",
      "preparing for single simulation took 1.1644 seconds\n",
      "single simulation took 0.13 minutes\n",
      "simulation with no output spikes. tossing a coin...\n",
      "decided to keep.\n",
      "\n",
      "\n",
      "data collection per single simulation took 0.0954 seconds\n",
      "-----------------------------------------------------------\n",
      "finished simulation 2: num output spikes = 0\n",
      "entire simulation took 0.15 minutes\n",
      "------------------------------------------------------------------------------/\n",
      "...\n",
      "------------------------------------------------------------------------------\\\n",
      "going to insert excitatory spikes per 100ms in range: [41, 133]\n",
      "going to insert inhibitory spikes per 100ms in range: [0, 203]\n",
      "preparing for single simulation took 1.1031 seconds\n",
      "single simulation took 0.11 minutes\n",
      "simulation with no output spikes. tossing a coin...\n",
      "decided to keep.\n",
      "\n",
      "\n",
      "data collection per single simulation took 0.0970 seconds\n",
      "-----------------------------------------------------------\n",
      "finished simulation 3: num output spikes = 0\n",
      "entire simulation took 0.13 minutes\n",
      "------------------------------------------------------------------------------/\n",
      "...\n",
      "------------------------------------------------------------------------------\\\n",
      "going to insert excitatory spikes per 100ms in range: [20, 129]\n",
      "going to insert inhibitory spikes per 100ms in range: [0, 186]\n",
      "preparing for single simulation took 1.1347 seconds\n",
      "single simulation took 0.13 minutes\n",
      "simulation with no output spikes. tossing a coin...\n",
      "decided to keep.\n",
      "\n",
      "\n",
      "data collection per single simulation took 0.0959 seconds\n",
      "-----------------------------------------------------------\n",
      "finished simulation 4: num output spikes = 0\n",
      "entire simulation took 0.15 minutes\n",
      "------------------------------------------------------------------------------/\n",
      "...\n",
      "...\n",
      "...\n",
      "-------------------------------------------------\\\n",
      "entire experiment took 0.54 minutes\n",
      "-----------------------------------------------\n",
      "total number of simulations is 4\n",
      "total number of collected spikes is 0\n",
      "average number of excitatory spikes per 100ms is: [21, 133]\n",
      "average number of inhibitory spikes per 100ms is: [0, 216]\n",
      "average output frequency is 0.00 [Hz]\n",
      "number of spikes per simulation minute is 0.00\n",
      "ISI-CV is nan\n",
      "-------------------------------------------------/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gili/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/gili/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:194: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(\n",
      "/Users/gili/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/gili/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/gili/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/ems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-471ddd195365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0mdirToSaveIn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenameToSave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dir_name_and_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_spikes_per_100ms_mean_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minh_spikes_per_100ms_mean_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalNumOutputSpikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirToSaveIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirToSaveIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;31m# pickle everythin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/ems'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import neuron\n",
    "from neuron import h\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import signal\n",
    "import pickle\n",
    "\n",
    "# get or randomly generate random seed\n",
    "try:\n",
    "    print('--------------')\n",
    "    random_seed = int(sys.argv[1])\n",
    "    morphology_description = sys.argv[2]\n",
    "    gmax_NMDA_to_AMPA_ratio = float(sys.argv[3])\n",
    "    print('\"random_seed\" selected by user - %d' %(random_seed))\n",
    "    print('\"morphology_description\" selected by user - %s' %(morphology_description))\n",
    "    print('\"gmax_NMDA_to_AMPA_ratio\" selected by user - %.3f' %(gmax_NMDA_to_AMPA_ratio))\n",
    "\n",
    "    determine_internally = False\n",
    "except:\n",
    "    determine_internally = True\n",
    "    try:\n",
    "        random_seed = int(sys.argv[1])\n",
    "        print('random seed selected by user - %d' %(random_seed))\n",
    "    except:\n",
    "        random_seed = np.random.randint(100000)\n",
    "        print('randomly choose seed - %d' %(random_seed))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "print('--------------')\n",
    "\n",
    "#%% define simulation params\n",
    "\n",
    "# general simulation parameters\n",
    "#numSimulations = 128\n",
    "numSimulations = 4\n",
    "totalSimDurationInSec = 6\n",
    "\n",
    "#collectAndSaveDVTs = False\n",
    "collectAndSaveDVTs = True\n",
    "numSamplesPerMS_HighRes = 8\n",
    "\n",
    "# model params\n",
    "if determine_internally:\n",
    "    morphology_description = 'basal_oblique_tuft'\n",
    "    #morphology_description = 'basal_oblique'\n",
    "    #morphology_description = 'basal_full'\n",
    "    #morphology_description = 'basal_proximal'\n",
    "    #morphology_description = 'basal_distal'\n",
    "    #morphology_description = 'basal_subtree_A'\n",
    "    #morphology_description = 'basal_subtree_B'\n",
    "    \n",
    "    # NMDA to AMPA conductance ratio\n",
    "    gmax_NMDA_to_AMPA_ratio = 1.0\n",
    "\n",
    "\n",
    "# SK channel g_max multiplication factor (0.0 - AMPA only, 1.0 - regular, 2.0 - human synapses)\n",
    "SKE2_mult_factor = 1.0\n",
    "\n",
    "# vshift of the Ih activation curve (can be in [-30,30])\n",
    "Ih_vshift = 0\n",
    "\n",
    "# some selection adaptive mechansim to keep simulations similar (in output rates) with different params\n",
    "keep_probability_below_01_output_spikes = 1.0\n",
    "keep_probability_above_24_output_spikes = 0.1\n",
    "max_output_spikes_to_keep_per_sim = 1\n",
    "\n",
    "# another mechansim to keep simulations similar (in output rates) with different number of active segments\n",
    "# 10% change per 131 active segments\n",
    "max_spikes_mult_factor_per_active_segment = 0.1 / 131\n",
    "\n",
    "# 40% change per 1.0 NMDA_to_AMPA_g_ratio\n",
    "max_spikes_mult_factor_per_NMDA_g_ratio = 0.4\n",
    "\n",
    "# 15% change per 1.0 NMDA_to_AMPA_g_ratio\n",
    "inh_max_delta_spikes_mult_factor_per_NMDA_g_ratio = 0.15\n",
    "\n",
    "# load spatial clustering matrix\n",
    "spatial_clusters_matrix_filename = os.path.join('spatial_clusters_matrix.npz')\n",
    "spatial_clusters_matrix = np.load(spatial_clusters_matrix_filename)['spatial_clusters_matrix']\n",
    "\n",
    "# load morphology subparts dictionary\n",
    "morphology_subparts_segment_inds_filename = os.path.join('morphology_subparts_segment_inds.p')\n",
    "morphology_subparts_segment_inds = pickle.load(open(morphology_subparts_segment_inds_filename, 'rb'))\n",
    "segments_to_keep = morphology_subparts_segment_inds[morphology_description]\n",
    "\n",
    "# calculate the input firing rate deflection from canonical case, depending on morphology and synapatic params\n",
    "num_active_segments = len(segments_to_keep)\n",
    "max_spikes_mult_factor_A = 1 - max_spikes_mult_factor_per_active_segment * (num_active_segments - 262)\n",
    "max_spikes_mult_factor_B = 1 - max_spikes_mult_factor_per_NMDA_g_ratio   * (gmax_NMDA_to_AMPA_ratio - 1)\n",
    "exc_max_spikes_mult_factor = max_spikes_mult_factor_A * max_spikes_mult_factor_B\n",
    "\n",
    "inh_max_delta_spikes_mult_factor = 1 + inh_max_delta_spikes_mult_factor_per_NMDA_g_ratio * (gmax_NMDA_to_AMPA_ratio - 1)\n",
    "\n",
    "# add an additional boost to those who need it\n",
    "if num_active_segments < 200:\n",
    "    exc_max_spikes_mult_factor  = 1.40 * exc_max_spikes_mult_factor\n",
    "\n",
    "if gmax_NMDA_to_AMPA_ratio < 0.6:\n",
    "    exc_max_spikes_mult_factor  = 1.10 * exc_max_spikes_mult_factor\n",
    "\n",
    "if num_active_segments < 200 and gmax_NMDA_to_AMPA_ratio < 0.6:\n",
    "    exc_max_spikes_mult_factor  = 1.15 * exc_max_spikes_mult_factor\n",
    "\n",
    "if num_active_segments < 200 and gmax_NMDA_to_AMPA_ratio < 0.3:\n",
    "    exc_max_spikes_mult_factor  = 1.15 * exc_max_spikes_mult_factor\n",
    "\n",
    "if gmax_NMDA_to_AMPA_ratio < 0.3:\n",
    "    exc_max_spikes_mult_factor       = 1.35 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 0.90 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if gmax_NMDA_to_AMPA_ratio > 1.6:\n",
    "    exc_max_spikes_mult_factor       = 1.1 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_proximal':\n",
    "    exc_max_spikes_mult_factor       = 1.05 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 0.95 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_subtree_B':\n",
    "    exc_max_spikes_mult_factor       = 0.95 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_proximal' and gmax_NMDA_to_AMPA_ratio > 0.3:\n",
    "    exc_max_spikes_mult_factor       = 1.1 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_proximal' and gmax_NMDA_to_AMPA_ratio < 0.6:\n",
    "    exc_max_spikes_mult_factor       = 0.85 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_subtree_A' and gmax_NMDA_to_AMPA_ratio < 1.1:\n",
    "    exc_max_spikes_mult_factor       = 1.10 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 0.94 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_subtree_A' and gmax_NMDA_to_AMPA_ratio < 0.6:\n",
    "    exc_max_spikes_mult_factor       = 1.10 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_subtree_B' and gmax_NMDA_to_AMPA_ratio > 0.9:\n",
    "    exc_max_spikes_mult_factor       = 0.94 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 1.06 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_subtree_B' and gmax_NMDA_to_AMPA_ratio > 1.1:\n",
    "    exc_max_spikes_mult_factor       = 0.94 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 1.06 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_distal' and gmax_NMDA_to_AMPA_ratio < 0.3:\n",
    "    exc_max_spikes_mult_factor       = 1.07 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_distal' and gmax_NMDA_to_AMPA_ratio < 0.6:\n",
    "    exc_max_spikes_mult_factor       = 1.05 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 0.95 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_distal' and gmax_NMDA_to_AMPA_ratio > 1.1:\n",
    "    exc_max_spikes_mult_factor       = 0.88 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 1.12 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_full' and gmax_NMDA_to_AMPA_ratio > 0.9:\n",
    "    exc_max_spikes_mult_factor       = 0.95 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 1.05 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description == 'basal_oblique' and gmax_NMDA_to_AMPA_ratio > 0.9:\n",
    "    exc_max_spikes_mult_factor       = 0.95 * exc_max_spikes_mult_factor\n",
    "    inh_max_delta_spikes_mult_factor = 1.05 * inh_max_delta_spikes_mult_factor\n",
    "\n",
    "if morphology_description in ['basal_distal', 'basal_subtree_A', 'basal_subtree_B'] and gmax_NMDA_to_AMPA_ratio < 0.3:\n",
    "    exc_max_spikes_mult_factor       = 1.1 * exc_max_spikes_mult_factor\n",
    "\n",
    "if morphology_description in ['basal_distal', 'basal_subtree_A'] and gmax_NMDA_to_AMPA_ratio < 0.3:\n",
    "    exc_max_spikes_mult_factor       = 1.1 * exc_max_spikes_mult_factor\n",
    "\n",
    "# input params\n",
    "\n",
    "# number of spike ranges for the simulation\n",
    "num_bas_ex_spikes_per_100ms_range          = [0, int(1350 * exc_max_spikes_mult_factor)]\n",
    "num_apic_ex_spikes_per_100ms_range         = [0, int(1400 * exc_max_spikes_mult_factor)]\n",
    "num_bas_ex_inh_spike_diff_per_100ms_range  = [-num_bas_ex_spikes_per_100ms_range[1] , int(600 * inh_max_delta_spikes_mult_factor)]\n",
    "num_apic_ex_inh_spike_diff_per_100ms_range = [-num_apic_ex_spikes_per_100ms_range[1], int(600 * inh_max_delta_spikes_mult_factor)]\n",
    "\n",
    "num_bas_ex_spikes_per_100ms_range          = [0, int(100)]\n",
    "num_apic_ex_spikes_per_100ms_range         = [0, int(125)]\n",
    "num_bas_ex_inh_spike_diff_per_100ms_range  = [-num_bas_ex_spikes_per_100ms_range[1] , int(100)]\n",
    "num_apic_ex_inh_spike_diff_per_100ms_range = [-num_apic_ex_spikes_per_100ms_range[1], int(125)]\n",
    "\n",
    "# define inst rate between change interval and smoothing sigma options (two rules of thumb:)\n",
    "# (A) increasing sampling time interval increases firing rate (more cumulative spikes at \"lucky high rate\" periods)\n",
    "# (B) increasing smoothing sigma reduces output firing rate (reduce effect of \"lucky high rate\" periods due to averaging)\n",
    "inst_rate_sampling_time_interval_options_ms   = [25,30,35,40,45,50,55,60,65,70,75,80,85,90,100,150,200,300,450]\n",
    "temporal_inst_rate_smoothing_sigma_options_ms = [25,30,35,40,45,50,55,60,65,80,100,150,200,250,300,400,500,600]\n",
    "inst_rate_sampling_time_interval_jitter_range   = 20\n",
    "temporal_inst_rate_smoothing_sigma_jitter_range = 20\n",
    "\n",
    "# spatial clustering params\n",
    "spatial_clustering_prob = 0.25\n",
    "spatial_clustering_prob = 0.0\n",
    "num_spatial_clusters_range = [0,6]\n",
    "num_active_spatial_clusters_range = [1,5]\n",
    "\n",
    "# synchronization\n",
    "synchronization_prob = 0.20\n",
    "synchronization_prob = 0.0\n",
    "synchronization_period_range = [30,200]\n",
    "\n",
    "# remove inhibition fraction\n",
    "remove_inhibition_prob = 0.30\n",
    "\n",
    "# \"regularization\" param for the segment lengths\n",
    "min_seg_length_um = 10.0\n",
    "\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('\"random_seed\" - %d' %(random_seed))\n",
    "print('\"morphology_description\" - %s' %(morphology_description))\n",
    "print('\"gmax_NMDA_to_AMPA_ratio\" - %.3f' %(gmax_NMDA_to_AMPA_ratio))\n",
    "print('-----------------------------------------')\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('spatial_clusters_matrix = ')\n",
    "print('-----------------------------------------')\n",
    "print(spatial_clusters_matrix)\n",
    "print('-----------------------------------------')\n",
    "\n",
    "print('-----------------------------------------')\n",
    "print('segments_to_keep = ')\n",
    "print('-----------------------------------------')\n",
    "print(segments_to_keep)\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# beaurrocracy\n",
    "showPlots = False\n",
    "resultsSavedIn_rootFolder = '/ems/elsc-labs/segev-i/david.beniaguev/Reseach/Single_Neuron_InOut/ExperimentalData/Neuron_Revision/Exp_AB_data/'\n",
    "\n",
    "# simulation duration\n",
    "sim_duration_sec = totalSimDurationInSec\n",
    "sim_duration_ms  = 1000 * sim_duration_sec\n",
    "\n",
    "useCvode = True\n",
    "totalSimDurationInMS = 1000 * totalSimDurationInSec\n",
    "\n",
    "#%% define some helper functions\n",
    "\n",
    "\n",
    "def generate_input_spike_rates_for_simulation(sim_duration_ms):\n",
    "\n",
    "    # extract the number of basal and apical segments\n",
    "    num_basal_segments  = len(basal_seg_length_um)\n",
    "    num_apical_segments = len(apical_seg_length_um)\n",
    "        \n",
    "    # adjust segment lengths (with \"min_seg_length_um\")\n",
    "    adjusted_basal_length_um  = min_seg_length_um + basal_seg_length_um\n",
    "    adjusted_apical_length_um = min_seg_length_um + apical_seg_length_um\n",
    "    \n",
    "    # calc sum of seg length (to be used for normalization later on)\n",
    "    total_adjusted_basal_tree_length_um  = adjusted_basal_length_um.sum()\n",
    "    total_adjusted_apical_tree_length_um = adjusted_apical_length_um.sum()\n",
    "    \n",
    "    # randomly sample inst rate (with some uniform noise) smoothing sigma\n",
    "    keep_inst_rate_const_for_ms = inst_rate_sampling_time_interval_options_ms[np.random.randint(len(inst_rate_sampling_time_interval_options_ms))]\n",
    "    keep_inst_rate_const_for_ms += int(2 * inst_rate_sampling_time_interval_jitter_range * np.random.rand() - inst_rate_sampling_time_interval_jitter_range)\n",
    "    \n",
    "    # randomly sample smoothing sigma (with some uniform noise)\n",
    "    temporal_inst_rate_smoothing_sigma = temporal_inst_rate_smoothing_sigma_options_ms[np.random.randint(len(temporal_inst_rate_smoothing_sigma_options_ms))]\n",
    "    temporal_inst_rate_smoothing_sigma += int(2 * temporal_inst_rate_smoothing_sigma_jitter_range * np.random.rand() - temporal_inst_rate_smoothing_sigma_jitter_range)\n",
    "    \n",
    "    num_inst_rate_samples = int(np.ceil(float(sim_duration_ms) / keep_inst_rate_const_for_ms))\n",
    "    \n",
    "    # create the coarse inst rates with units of \"total spikes per tree per 100 ms\"\n",
    "    num_bas_ex_spikes_per_100ms   = np.random.uniform(low=num_bas_ex_spikes_per_100ms_range[0], high=num_bas_ex_spikes_per_100ms_range[1], size=(1,num_inst_rate_samples))\n",
    "    num_bas_inh_spikes_low_range  = np.maximum(0, num_bas_ex_spikes_per_100ms + num_bas_ex_inh_spike_diff_per_100ms_range[0])\n",
    "    num_bas_inh_spikes_high_range = num_bas_ex_spikes_per_100ms + num_bas_ex_inh_spike_diff_per_100ms_range[1]\n",
    "    num_bas_inh_spikes_per_100ms  = np.random.uniform(low=num_bas_inh_spikes_low_range, high=num_bas_inh_spikes_high_range, size=(1,num_inst_rate_samples))\n",
    "    \n",
    "    num_apic_ex_spikes_per_100ms   = np.random.uniform(low=num_apic_ex_spikes_per_100ms_range[0], high=num_apic_ex_spikes_per_100ms_range[1],size=(1,num_inst_rate_samples))\n",
    "    num_apic_inh_spikes_low_range  = np.maximum(0, num_apic_ex_spikes_per_100ms + num_apic_ex_inh_spike_diff_per_100ms_range[0])\n",
    "    num_apic_inh_spikes_high_range = num_apic_ex_spikes_per_100ms + num_apic_ex_inh_spike_diff_per_100ms_range[1]\n",
    "    num_apic_inh_spikes_per_100ms  = np.random.uniform(low=num_apic_inh_spikes_low_range, high=num_apic_inh_spikes_high_range, size=(1,num_inst_rate_samples))\n",
    "    \n",
    "    # convert to units of \"per_1um_per_1ms\"\n",
    "    ex_bas_spike_rate_per_1um_per_1ms   = num_bas_ex_spikes_per_100ms   / (total_adjusted_basal_tree_length_um  * 100.0)\n",
    "    inh_bas_spike_rate_per_1um_per_1ms  = num_bas_inh_spikes_per_100ms  / (total_adjusted_basal_tree_length_um  * 100.0)\n",
    "    ex_apic_spike_rate_per_1um_per_1ms  = num_apic_ex_spikes_per_100ms  / (total_adjusted_apical_tree_length_um * 100.0)\n",
    "    inh_apic_spike_rate_per_1um_per_1ms = num_apic_inh_spikes_per_100ms / (total_adjusted_apical_tree_length_um * 100.0)\n",
    "            \n",
    "    # kron by space (uniform distribution across branches per tree)\n",
    "    ex_bas_spike_rate_per_seg_per_1ms   = np.kron(ex_bas_spike_rate_per_1um_per_1ms  , np.ones((num_basal_segments,1)))\n",
    "    inh_bas_spike_rate_per_seg_per_1ms  = np.kron(inh_bas_spike_rate_per_1um_per_1ms , np.ones((num_basal_segments,1)))\n",
    "    ex_apic_spike_rate_per_seg_per_1ms  = np.kron(ex_apic_spike_rate_per_1um_per_1ms , np.ones((num_apical_segments,1)))\n",
    "    inh_apic_spike_rate_per_seg_per_1ms = np.kron(inh_apic_spike_rate_per_1um_per_1ms, np.ones((num_apical_segments,1)))\n",
    "        \n",
    "    # vstack basal and apical\n",
    "    ex_spike_rate_per_seg_per_1ms  = np.vstack((ex_bas_spike_rate_per_seg_per_1ms , ex_apic_spike_rate_per_seg_per_1ms))\n",
    "    inh_spike_rate_per_seg_per_1ms = np.vstack((inh_bas_spike_rate_per_seg_per_1ms, inh_apic_spike_rate_per_seg_per_1ms))\n",
    "    \n",
    "    # add some spatial multiplicative randomness (that will be added to the sampling noise)\n",
    "    ex_spike_rate_per_seg_per_1ms  = np.random.uniform(low=0.5, high=1.5, size=ex_spike_rate_per_seg_per_1ms.shape ) * ex_spike_rate_per_seg_per_1ms\n",
    "    inh_spike_rate_per_seg_per_1ms = np.random.uniform(low=0.5, high=1.5, size=inh_spike_rate_per_seg_per_1ms.shape) * inh_spike_rate_per_seg_per_1ms\n",
    "    \n",
    "    # concatenate the adjusted length\n",
    "    adjusted_length_um = np.hstack((adjusted_basal_length_um, adjusted_apical_length_um))\n",
    "    \n",
    "    # multiply each segment by it's length (now every segment will have firing rate proportional to it's length)\n",
    "    ex_spike_rate_per_seg_per_1ms  = ex_spike_rate_per_seg_per_1ms  * np.tile(adjusted_length_um[:,np.newaxis], [1, ex_spike_rate_per_seg_per_1ms.shape[1]])\n",
    "    inh_spike_rate_per_seg_per_1ms = inh_spike_rate_per_seg_per_1ms * np.tile(adjusted_length_um[:,np.newaxis], [1, inh_spike_rate_per_seg_per_1ms.shape[1]])\n",
    "        \n",
    "    # kron by time (crop if there are leftovers in the end) to fill up the time to 1ms time bins\n",
    "    ex_spike_rate_per_seg_per_1ms  = np.kron(ex_spike_rate_per_seg_per_1ms , np.ones((1,keep_inst_rate_const_for_ms)))[:,:sim_duration_ms]\n",
    "    inh_spike_rate_per_seg_per_1ms = np.kron(inh_spike_rate_per_seg_per_1ms, np.ones((1,keep_inst_rate_const_for_ms)))[:,:sim_duration_ms]\n",
    "    \n",
    "    # filter the inst rates according to smoothing sigma\n",
    "    smoothing_window = signal.gaussian(1.0 + 7 * temporal_inst_rate_smoothing_sigma, std=temporal_inst_rate_smoothing_sigma)[np.newaxis,:]\n",
    "    smoothing_window /= smoothing_window.sum()\n",
    "    seg_inst_rate_ex_smoothed  = signal.convolve(ex_spike_rate_per_seg_per_1ms,  smoothing_window, mode='same')\n",
    "    seg_inst_rate_inh_smoothed = signal.convolve(inh_spike_rate_per_seg_per_1ms, smoothing_window, mode='same')\n",
    "\n",
    "    # add synchronization if necessary\n",
    "    if np.random.rand() < synchronization_prob:\n",
    "        synchronization_period = np.random.randint(synchronization_period_range[0], synchronization_period_range[1])\n",
    "        time_ms = np.arange(0, sim_duration_ms)\n",
    "        temporal_profile = 0.6 * np.sin(2 * np.pi * time_ms / synchronization_period) + 1.0\n",
    "        temp_mult_factor = np.tile(temporal_profile[np.newaxis], (seg_inst_rate_ex_smoothed.shape[0], 1))\n",
    "\n",
    "        seg_inst_rate_ex_smoothed  = temp_mult_factor * seg_inst_rate_ex_smoothed\n",
    "        seg_inst_rate_inh_smoothed = temp_mult_factor * seg_inst_rate_inh_smoothed\n",
    "\n",
    "    # remove inhibition if necessary\n",
    "    if np.random.rand() < remove_inhibition_prob:\n",
    "        # reduce inhibition to zero\n",
    "        seg_inst_rate_inh_smoothed[:] = 0\n",
    "\n",
    "        # reduce average excitatory firing rate\n",
    "        excitation_mult_factor = 0.10 + 0.40 * np.random.rand()\n",
    "        seg_inst_rate_ex_smoothed = excitation_mult_factor * seg_inst_rate_ex_smoothed\n",
    "\n",
    "    # add spatial clustering througout the entire simulation\n",
    "    if np.random.rand() < spatial_clustering_prob:\n",
    "        spatial_cluster_matrix_row = np.random.randint(num_spatial_clusters_range[0], num_spatial_clusters_range[1])\n",
    "        curr_clustering_row = spatial_clusters_matrix[spatial_cluster_matrix_row,:]\n",
    "        num_spatial_clusters = np.unique(curr_clustering_row).shape[0]\n",
    "\n",
    "        max_num_active_clusters = max(2, min(int(0.4 * num_spatial_clusters), num_active_spatial_clusters_range[1]))\n",
    "        num_active_clusters = np.random.randint(num_active_spatial_clusters_range[0], max_num_active_clusters)\n",
    "\n",
    "        active_clusters = np.random.choice(np.unique(curr_clustering_row), size=num_active_clusters)\n",
    "        spatial_mult_factor = np.tile(np.isin(curr_clustering_row, active_clusters)[:,np.newaxis], (1, seg_inst_rate_ex_smoothed.shape[1]))\n",
    "\n",
    "        seg_inst_rate_ex_smoothed  = spatial_mult_factor * seg_inst_rate_ex_smoothed\n",
    "        seg_inst_rate_inh_smoothed = spatial_mult_factor * seg_inst_rate_inh_smoothed\n",
    "\n",
    "    return seg_inst_rate_ex_smoothed, seg_inst_rate_inh_smoothed\n",
    "\n",
    "\n",
    "def sample_spikes_from_rates(seg_inst_rate_ex, seg_inst_rate_inh):\n",
    "\n",
    "    # sample the instantanous spike prob and then sample the actual spikes\n",
    "    ex_inst_spike_prob = np.random.exponential(scale=seg_inst_rate_ex)\n",
    "    exc_spikes_bin      = np.random.rand(ex_inst_spike_prob.shape[0], ex_inst_spike_prob.shape[1]) < ex_inst_spike_prob\n",
    "    \n",
    "    inh_inst_spike_prob = np.random.exponential(scale=seg_inst_rate_inh)\n",
    "    inh_spikes_bin      = np.random.rand(inh_inst_spike_prob.shape[0], inh_inst_spike_prob.shape[1]) < inh_inst_spike_prob\n",
    "    \n",
    "    return exc_spikes_bin, inh_spikes_bin\n",
    "\n",
    "\n",
    "def generate_input_spike_trains_for_simulation_new(sim_duration_ms, transition_dur_ms=25, num_segments=5, segment_dur_ms=1500):\n",
    "\n",
    "    inst_rate_exc, inst_rate_inh = generate_input_spike_rates_for_simulation(sim_duration_ms)\n",
    "    segment_added_egde_indicator = np.zeros(sim_duration_ms)\n",
    "    for k in range(num_segments):\n",
    "        segment_start_ind = np.random.randint(sim_duration_ms - segment_dur_ms - 10)\n",
    "        segment_duration_ms = np.random.randint(500, segment_dur_ms)\n",
    "        segment_final_ind = segment_start_ind + segment_duration_ms\n",
    "\n",
    "        curr_seg_inst_rate_exc, curr_seg_inst_rate_inh = generate_input_spike_rates_for_simulation(segment_duration_ms)\n",
    "\n",
    "        inst_rate_exc[:,segment_start_ind:segment_final_ind] = curr_seg_inst_rate_exc\n",
    "        inst_rate_inh[:,segment_start_ind:segment_final_ind] = curr_seg_inst_rate_inh\n",
    "        segment_added_egde_indicator[segment_start_ind] = 1\n",
    "        segment_added_egde_indicator[segment_final_ind] = 1\n",
    "\n",
    "    smoothing_window = signal.gaussian(1.0 + 7 * transition_dur_ms, std=transition_dur_ms)\n",
    "    segment_added_egde_indicator = signal.convolve(segment_added_egde_indicator,  smoothing_window, mode='same') > 0.2\n",
    "\n",
    "    smoothing_window /= smoothing_window.sum()\n",
    "    seg_inst_rate_exc_smoothed = signal.convolve(inst_rate_exc, smoothing_window[np.newaxis,:], mode='same')\n",
    "    seg_inst_rate_inh_smoothed = signal.convolve(inst_rate_inh, smoothing_window[np.newaxis,:], mode='same')\n",
    "\n",
    "    # build the final rates matrices\n",
    "    inst_rate_exc_final = inst_rate_exc.copy()\n",
    "    inst_rate_inh_final = inst_rate_inh.copy()\n",
    "\n",
    "    inst_rate_exc_final[:,segment_added_egde_indicator] = seg_inst_rate_exc_smoothed[:,segment_added_egde_indicator]\n",
    "    inst_rate_inh_final[:,segment_added_egde_indicator] = seg_inst_rate_inh_smoothed[:,segment_added_egde_indicator]\n",
    "\n",
    "    # correct any minor mistakes\n",
    "    inst_rate_exc_final[inst_rate_exc_final <= 0] = 0\n",
    "    inst_rate_inh_final[inst_rate_inh_final <= 0] = 0\n",
    "\n",
    "    exc_spikes_bin, inh_spikes_bin = sample_spikes_from_rates(inst_rate_exc_final, inst_rate_inh_final)\n",
    "\n",
    "    return exc_spikes_bin, inh_spikes_bin\n",
    "\n",
    "\n",
    "def get_dir_name_and_filename(exc_range_per_100ms, inh_range_per_100ms, num_output_spikes, random_seed):\n",
    "\n",
    "    # string to describe model\n",
    "    synapse_type = 'NMDA_g_ratio_%0.3d' %(100 * gmax_NMDA_to_AMPA_ratio)\n",
    "    ephys_type   = 'Ih_vshift_%0.2d_SKE2_mult_%0.3d' %(Ih_vshift, 100 * SKE2_mult_factor)\n",
    "    morph_type   = morphology_description\n",
    "    model_string = 'L5PC__' + ephys_type + '__' + morph_type + '__' + synapse_type\n",
    "\n",
    "\n",
    "    # string to describe input\n",
    "    input_string = 'Exc_[%0.4d,%0.4d]_Inh_[%0.4d,%0.4d]_per100ms' %(exc_range_per_100ms[0], exc_range_per_100ms[1],\n",
    "                                                                    inh_range_per_100ms[0], inh_range_per_100ms[1])\n",
    "\n",
    "    # string to describe simulation\n",
    "    simulation_template_string = 'L5PC_sim__Output_spikes_%0.4d__Input_ranges_%s__simXsec_%dx%d_randseed_%d.p'\n",
    "    simulation_template_string = 'L5PC_sim__Output_spikes_%0.4d__Input_ranges_%s__simXsec_%dx%d_randseed_%d_subthreshold.p'\n",
    "\n",
    "    # output dir and filename\n",
    "    dir_to_save_in = resultsSavedIn_rootFolder + model_string + '_subthreshold/'\n",
    "    filename_to_save = simulation_template_string %(num_output_spikes, input_string, numSimulations, totalSimDurationInSec, random_seed)\n",
    "\n",
    "    return dir_to_save_in, filename_to_save\n",
    "\n",
    "\n",
    "def GetDistanceBetweenSections(sourceSection, destSection):\n",
    "    h.distance(sec=sourceSection)\n",
    "    return h.distance(0, sec=destSection)\n",
    "\n",
    "\n",
    "# NMDA synapse\n",
    "def DefineSynapse_NMDA(segment, gMax=0.0004, NMDA_to_AMPA_g_ratio=1.0):\n",
    "    synapse = h.ProbAMPANMDA_David(segment)\n",
    "\n",
    "    synapse.tau_r_AMPA = 0.3\n",
    "    synapse.tau_d_AMPA = 3.0\n",
    "    synapse.tau_r_NMDA = 2.0\n",
    "    synapse.tau_d_NMDA = 70.0\n",
    "    synapse.gmax_AMPA = gMax\n",
    "    synapse.gmax_NMDA = gMax * NMDA_to_AMPA_g_ratio\n",
    "    synapse.e = 0\n",
    "    synapse.Use = 1\n",
    "    synapse.u0 = 0\n",
    "    synapse.Dep = 0\n",
    "    synapse.Fac = 0\n",
    "\n",
    "    return synapse\n",
    "\n",
    "\n",
    "# GABA A synapse\n",
    "def DefineSynapse_GABA_A(segment, gMax=0.001):\n",
    "    synapse = h.ProbUDFsyn2(segment)\n",
    "\n",
    "    synapse.tau_r = 0.2\n",
    "    synapse.tau_d = 8\n",
    "    synapse.gmax = gMax\n",
    "    synapse.e = -80\n",
    "    synapse.Use = 1\n",
    "    synapse.u0 = 0\n",
    "    synapse.Dep = 0\n",
    "    synapse.Fac = 0\n",
    "\n",
    "    return synapse\n",
    "\n",
    "\n",
    "def ConnectEmptyEventGenerator(synapse):\n",
    "\n",
    "    netConnection = h.NetCon(None,synapse)\n",
    "    netConnection.delay = 0\n",
    "    netConnection.weight[0] = 1\n",
    "\n",
    "    return netConnection\n",
    "\n",
    "\n",
    "#%% define model\n",
    "\n",
    "\n",
    "h.load_file('nrngui.hoc')\n",
    "h.load_file(\"import3d.hoc\")\n",
    "\n",
    "morphologyFilename = \"morphologies/cell1.asc\"\n",
    "biophysicalModelFilename = \"L5PCbiophysWhat.hoc\"\n",
    "biophysicalModelTemplateFilename = \"L5PCtemplate_2.hoc\"\n",
    "\n",
    "h.load_file(biophysicalModelFilename)\n",
    "h.load_file(biophysicalModelTemplateFilename)\n",
    "L5PC = h.L5PCtemplate(morphologyFilename)\n",
    "\n",
    "cvode = h.CVode()\n",
    "if useCvode:\n",
    "    cvode.active(1)\n",
    "\n",
    "#%% collect everything we need about the model\n",
    "\n",
    "# Get a list of all sections\n",
    "listOfBasalSections  = [L5PC.dend[x] for x in range(len(L5PC.dend))]\n",
    "listOfApicalSections = [L5PC.apic[x] for x in range(len(L5PC.apic))]\n",
    "allSections = listOfBasalSections + listOfApicalSections\n",
    "allSectionsType = ['basal' for x in listOfBasalSections] + ['apical' for x in listOfApicalSections]\n",
    "allSectionsLength = []\n",
    "allSections_DistFromSoma = []\n",
    "\n",
    "allSegments = []\n",
    "allSegmentsLength = []\n",
    "allSegmentsType = []\n",
    "allSegments_DistFromSoma = []\n",
    "allSegments_SectionDistFromSoma = []\n",
    "allSegments_SectionInd = []\n",
    "# get a list of all segments\n",
    "for k, section in enumerate(allSections):\n",
    "    allSectionsLength.append(section.L)\n",
    "    allSections_DistFromSoma.append(GetDistanceBetweenSections(L5PC.soma[0], section))\n",
    "    for currSegment in section:\n",
    "        allSegments.append(currSegment)\n",
    "        allSegmentsLength.append(float(section.L) / section.nseg)\n",
    "        allSegmentsType.append(allSectionsType[k])\n",
    "        allSegments_DistFromSoma.append(GetDistanceBetweenSections(L5PC.soma[0], section) + float(section.L) * currSegment.x)\n",
    "        allSegments_SectionDistFromSoma.append(GetDistanceBetweenSections(L5PC.soma[0], section))\n",
    "        allSegments_SectionInd.append(k)\n",
    "\n",
    "\n",
    "# set Ih vshift value and SK multiplicative factor\n",
    "for section in allSections:\n",
    "    section.vshift_Ih = Ih_vshift\n",
    "L5PC.soma[0].vshift_Ih = Ih_vshift\n",
    "\n",
    "list_of_axonal_sections = [L5PC.axon[x] for x in range(len(L5PC.axon))]\n",
    "list_of_somatic_sections = [L5PC.soma[x] for x in range(len(L5PC.soma))]\n",
    "all_sections_with_SKE2 = list_of_somatic_sections + listOfApicalSections\n",
    "\n",
    "print('-----------------------')\n",
    "for section in all_sections_with_SKE2:\n",
    "    orig_SKE2_g = section.gSK_E2bar_SK_E2\n",
    "    new_SKE2_g = orig_SKE2_g * SKE2_mult_factor\n",
    "    section.gSK_E2bar_SK_E2 = new_SKE2_g\n",
    "    \n",
    "    #print('SKE2 conductance before update = %.10f' %(orig_SKE2_g))\n",
    "    #print('SKE2 conductance after  update = %.10f (exprected)' %(new_SKE2_g))\n",
    "    #print('SKE2 conductance after  update = %.10f (actual)' %(section.gSK_E2bar_SK_E2))\n",
    "print('-----------------------')\n",
    "\n",
    "# Calculate total dendritic length\n",
    "numBasalSegments = 0\n",
    "numApicalSegments = 0\n",
    "totalBasalDendriticLength = 0\n",
    "totalApicalDendriticLength = 0\n",
    "\n",
    "basal_seg_length_um = []\n",
    "apical_seg_length_um = []\n",
    "for k, segmentLength in enumerate(allSegmentsLength):\n",
    "    if allSegmentsType[k] == 'basal':\n",
    "        basal_seg_length_um.append(segmentLength)\n",
    "        totalBasalDendriticLength += segmentLength\n",
    "        numBasalSegments += 1\n",
    "    if allSegmentsType[k] == 'apical':\n",
    "        apical_seg_length_um.append(segmentLength)\n",
    "        totalApicalDendriticLength += segmentLength\n",
    "        numApicalSegments += 1\n",
    "\n",
    "totalDendriticLength = sum(allSectionsLength)\n",
    "totalNumSegments = len(allSegments)\n",
    "\n",
    "# extract basal and apical segment lengths\n",
    "num_basal_segments  = len(basal_seg_length_um)\n",
    "num_apical_segments = len(apical_seg_length_um)\n",
    "\n",
    "basal_seg_length_um = np.array(basal_seg_length_um)\n",
    "apical_seg_length_um = np.array(apical_seg_length_um)\n",
    "segments_to_drop = np.array(list(set(np.arange(totalNumSegments)).difference(set(segments_to_keep)))).astype(int)\n",
    "\n",
    "print('-----------------')\n",
    "print('segments_to_drop:')\n",
    "print('-----------------')\n",
    "print(segments_to_drop.shape)\n",
    "print(segments_to_drop)\n",
    "print('-----------------')\n",
    "\n",
    "assert(totalNumSegments == (numBasalSegments + numApicalSegments))\n",
    "assert(abs(totalDendriticLength - (totalBasalDendriticLength + totalApicalDendriticLength)) < 0.00001)\n",
    "\n",
    "totalNumOutputSpikes = 0\n",
    "listOfISIs = []\n",
    "numOutputSpikesPerSim = []\n",
    "listOfSingleSimulationDicts = []\n",
    "exc_spikes_per_100ms_range_per_sim = []\n",
    "inh_spikes_per_100ms_range_per_sim = []\n",
    "\n",
    "##%% run the simulation\n",
    "experimentStartTime = time.time()\n",
    "print('-------------------------------------\\\\')\n",
    "print('temperature is %.2f degrees celsius' %(h.celsius))\n",
    "print('dt is %.4f ms' %(h.dt))\n",
    "print('-------------------------------------/')\n",
    "\n",
    "simInd = 0\n",
    "while simInd < numSimulations:\n",
    "    currSimulationResultsDict = {}\n",
    "    preparationStartTime = time.time()\n",
    "    print('...')\n",
    "    print('------------------------------------------------------------------------------\\\\')\n",
    "\n",
    "    exc_spikes_bin, inh_spikes_bin = generate_input_spike_trains_for_simulation_new(sim_duration_ms)\n",
    "\n",
    "    # zero out the necessary indices according to \"morphology_description\"\n",
    "    exc_spikes_bin[segments_to_drop,:] = 0\n",
    "    inh_spikes_bin[segments_to_drop,:] = 0\n",
    "\n",
    "    # calculate the empirical range of number exc and inh spikes per 100ms\n",
    "    exc_spikes_cumsum = exc_spikes_bin.sum(axis=0).cumsum()\n",
    "    exc_spikes_per_100ms = exc_spikes_cumsum[100:] - exc_spikes_cumsum[:-100]\n",
    "    exc_spikes_per_100ms_range = [int(np.percentile(exc_spikes_per_100ms, 5)), int(np.percentile(exc_spikes_per_100ms, 95))]\n",
    "    inh_spikes_cumsum = inh_spikes_bin.sum(axis=0).cumsum()\n",
    "    inh_spikes_per_100ms = inh_spikes_cumsum[100:] - inh_spikes_cumsum[:-100]\n",
    "    inh_spikes_per_100ms_range = [int(np.percentile(inh_spikes_per_100ms, 5)), int(np.percentile(inh_spikes_per_100ms, 95))]\n",
    "\n",
    "    print('going to insert excitatory spikes per 100ms in range: %s' %(str(exc_spikes_per_100ms_range)))\n",
    "    print('going to insert inhibitory spikes per 100ms in range: %s' %(str(inh_spikes_per_100ms_range)))\n",
    "\n",
    "    inputSpikeTrains_ex  = exc_spikes_bin\n",
    "    inputSpikeTrains_inh = inh_spikes_bin\n",
    "        \n",
    "    ##%% convert binary vectors to dict of spike times for each seg ind\n",
    "    exSpikeSegInds, exSpikeTimes = np.nonzero(inputSpikeTrains_ex)\n",
    "    exSpikeTimesMap = {}\n",
    "    for segInd, synTime in zip(exSpikeSegInds,exSpikeTimes):\n",
    "        if segInd in exSpikeTimesMap.keys():\n",
    "            exSpikeTimesMap[segInd].append(synTime)\n",
    "        else:\n",
    "            exSpikeTimesMap[segInd] = [synTime]\n",
    "    \n",
    "    inhSpikeSegInds, inhSpikeTimes = np.nonzero(inputSpikeTrains_inh)\n",
    "    inhSpikeTimesMap = {}\n",
    "    for segInd, synTime in zip(inhSpikeSegInds,inhSpikeTimes):\n",
    "        if segInd in inhSpikeTimesMap.keys():\n",
    "            inhSpikeTimesMap[segInd].append(synTime)\n",
    "        else:\n",
    "            inhSpikeTimesMap[segInd] = [synTime]\n",
    "\n",
    "    ##%% run simulation ########################\n",
    "    allExNetCons = []\n",
    "    allExNetConEventLists = []\n",
    "    \n",
    "    allInhNetCons = []\n",
    "    allInhNetConEventLists = []\n",
    "    \n",
    "    allExSynapses = []\n",
    "    allInhSynapses = []\n",
    "    \n",
    "    for segInd, segment in enumerate(allSegments):\n",
    "\n",
    "        ###### excitation ######\n",
    "\n",
    "        # define synapse and connect it to a segment\n",
    "        exSynapse = DefineSynapse_NMDA(segment, NMDA_to_AMPA_g_ratio=gmax_NMDA_to_AMPA_ratio)\n",
    "        allExSynapses.append(exSynapse)\n",
    "    \n",
    "        # connect synapse\n",
    "        netConnection = ConnectEmptyEventGenerator(exSynapse)\n",
    "\n",
    "        # update lists\n",
    "        allExNetCons.append(netConnection)\n",
    "        if segInd in exSpikeTimesMap.keys():\n",
    "            allExNetConEventLists.append(exSpikeTimesMap[segInd])\n",
    "        else:\n",
    "            allExNetConEventLists.append([])  # insert empty list if no event\n",
    "            \n",
    "        ###### inhibition ######\n",
    "    \n",
    "        # define synapse and connect it to a segment\n",
    "        inhSynapse = DefineSynapse_GABA_A(segment)\n",
    "        allInhSynapses.append(inhSynapse)\n",
    "    \n",
    "        # connect synapse\n",
    "        netConnection = ConnectEmptyEventGenerator(inhSynapse)\n",
    "    \n",
    "        # update lists\n",
    "        allInhNetCons.append(netConnection)\n",
    "        if segInd in inhSpikeTimesMap.keys():\n",
    "            allInhNetConEventLists.append(inhSpikeTimesMap[segInd])\n",
    "        else:\n",
    "            allInhNetConEventLists.append([])  # insert empty list if no event\n",
    "    \n",
    "    # define function to be run at the begining of the simulation to add synaptic events\n",
    "    def AddAllSynapticEvents():\n",
    "        for exNetCon, eventsList in zip(allExNetCons,allExNetConEventLists):\n",
    "            for eventTime in eventsList:\n",
    "                exNetCon.event(eventTime)\n",
    "        for inhNetCon, eventsList in zip(allInhNetCons,allInhNetConEventLists):\n",
    "            for eventTime in eventsList:\n",
    "                inhNetCon.event(eventTime)\n",
    "\n",
    "    # add voltage and time recordings\n",
    "                \n",
    "    # record time\n",
    "    recTime = h.Vector()\n",
    "    recTime.record(h._ref_t)\n",
    "    \n",
    "    # record soma voltage\n",
    "    recVoltageSoma = h.Vector()\n",
    "    recVoltageSoma.record(L5PC.soma[0](0.5)._ref_v)\n",
    "    \n",
    "    # record nexus voltage\n",
    "    nexusSectionInd = 50\n",
    "    recVoltageNexus = h.Vector()\n",
    "    recVoltageNexus.record(L5PC.apic[nexusSectionInd](0.9)._ref_v)\n",
    "    \n",
    "    # record all segments voltage\n",
    "    if collectAndSaveDVTs:\n",
    "        recVoltage_allSegments = []\n",
    "        for segInd, segment in enumerate(allSegments):\n",
    "            voltageRecSegment = h.Vector()\n",
    "            voltageRecSegment.record(segment._ref_v)\n",
    "            recVoltage_allSegments.append(voltageRecSegment)\n",
    "        \n",
    "    preparationDurationInSeconds = time.time() - preparationStartTime\n",
    "    print(\"preparing for single simulation took %.4f seconds\" % (preparationDurationInSeconds))\n",
    "\n",
    "    ##%% simulate the cell\n",
    "    simulationStartTime = time.time()\n",
    "    # make sure the following line will be run after h.finitialize()\n",
    "    fih = h.FInitializeHandler('nrnpython(\"AddAllSynapticEvents()\")')\n",
    "    h.finitialize(-76)\n",
    "    h.continuerun(totalSimDurationInMS)\n",
    "    singleSimulationDurationInMinutes = (time.time() - simulationStartTime) / 60\n",
    "    print(\"single simulation took %.2f minutes\" % (singleSimulationDurationInMinutes))\n",
    "\n",
    "    ##%% extract the params from the simulation\n",
    "    # collect all relevent recoding vectors (input spike times, dendritic voltage traces, soma voltage trace)\n",
    "    collectionStartTime = time.time()\n",
    "        \n",
    "    origRecordingTime = np.array(recTime.as_numpy())\n",
    "    origSomaVoltage   = np.array(recVoltageSoma.as_numpy())\n",
    "    origNexusVoltage  = np.array(recVoltageNexus.as_numpy())\n",
    "    \n",
    "    # high res - origNumSamplesPerMS per ms\n",
    "    recordingTimeHighRes = np.arange(0, totalSimDurationInMS, 1.0 / numSamplesPerMS_HighRes)\n",
    "    somaVoltageHighRes   = np.interp(recordingTimeHighRes, origRecordingTime, origSomaVoltage)\n",
    "    nexusVoltageHighRes  = np.interp(recordingTimeHighRes, origRecordingTime, origNexusVoltage)\n",
    "\n",
    "    # low res - 1 sample per ms\n",
    "    recordingTimeLowRes = np.arange(0,totalSimDurationInMS)\n",
    "    somaVoltageLowRes   = np.interp(recordingTimeLowRes, origRecordingTime, origSomaVoltage)\n",
    "    nexusVoltageLowRes  = np.interp(recordingTimeLowRes, origRecordingTime, origNexusVoltage)\n",
    "    \n",
    "    if collectAndSaveDVTs:\n",
    "        dendriticVoltages = np.zeros((len(recVoltage_allSegments),recordingTimeLowRes.shape[0]))\n",
    "        for segInd, recVoltageSeg in enumerate(recVoltage_allSegments):\n",
    "            dendriticVoltages[segInd,:] = np.interp(recordingTimeLowRes, origRecordingTime, np.array(recVoltageSeg.as_numpy()))\n",
    "\n",
    "    # detect soma spike times\n",
    "    risingBefore = np.hstack((0, somaVoltageHighRes[1:] - somaVoltageHighRes[:-1])) > 0\n",
    "    fallingAfter = np.hstack((somaVoltageHighRes[1:] - somaVoltageHighRes[:-1], 0)) < 0\n",
    "    localMaximum = np.logical_and(fallingAfter, risingBefore)\n",
    "    largerThanThresh = somaVoltageHighRes > -25\n",
    "    \n",
    "    binarySpikeVector = np.logical_and(localMaximum,largerThanThresh)\n",
    "    spikeInds = np.nonzero(binarySpikeVector)\n",
    "    outputSpikeTimes = recordingTimeHighRes[spikeInds]\n",
    "    numOutputSpikes = len(outputSpikeTimes)\n",
    "\n",
    "    # check if the simulation has too few (< 1) output spikes\n",
    "    if numOutputSpikes < 1:\n",
    "        print('simulation with no output spikes. tossing a coin...')\n",
    "        if np.random.rand() < keep_probability_below_01_output_spikes:\n",
    "            print('decided to keep.\\n\\n')\n",
    "        else:\n",
    "            print('decided to not save. continue\\n\\n')\n",
    "            continue\n",
    "\n",
    "    # check if the simulation has too many (> 24) output spikes\n",
    "    if numOutputSpikes > 24:\n",
    "        print('simulation with many (%d) output spikes. tossing a coin...' %(numOutputSpikes))\n",
    "        if np.random.rand() < keep_probability_above_24_output_spikes:\n",
    "            print('decided to keep.\\n\\n')\n",
    "        else:\n",
    "            print('decided to not save. continue\\n\\n')\n",
    "            continue\n",
    "\n",
    "    # check if the simulation has too many output spikes\n",
    "    if numOutputSpikes > max_output_spikes_to_keep_per_sim:\n",
    "        print('simulation with too many spikes (%d). droping it\\n\\n' %(numOutputSpikes))\n",
    "        continue\n",
    "\n",
    "    # store everything that needs to be stored\n",
    "    currSimulationResultsDict['recordingTimeHighRes'] = recordingTimeHighRes.astype(np.float32)\n",
    "    currSimulationResultsDict['somaVoltageHighRes']   = somaVoltageHighRes.astype(np.float16)\n",
    "    currSimulationResultsDict['nexusVoltageHighRes']  = nexusVoltageHighRes.astype(np.float16)\n",
    "    \n",
    "    currSimulationResultsDict['recordingTimeLowRes'] = recordingTimeLowRes.astype(np.float32)\n",
    "    currSimulationResultsDict['somaVoltageLowRes']   = somaVoltageLowRes.astype(np.float16)\n",
    "    currSimulationResultsDict['nexusVoltageLowRes']  = nexusVoltageLowRes.astype(np.float16)\n",
    "\n",
    "    currSimulationResultsDict['exInputSpikeTimes']  = exSpikeTimesMap\n",
    "    currSimulationResultsDict['inhInputSpikeTimes'] = inhSpikeTimesMap\n",
    "    currSimulationResultsDict['outputSpikeTimes']   = outputSpikeTimes.astype(np.float16)\n",
    "    \n",
    "    if collectAndSaveDVTs:\n",
    "        currSimulationResultsDict['dendriticVoltagesLowRes'] = dendriticVoltages.astype(np.float16)\n",
    "\n",
    "    exc_spikes_per_100ms_range_per_sim.append(exc_spikes_per_100ms_range)\n",
    "    inh_spikes_per_100ms_range_per_sim.append(inh_spikes_per_100ms_range)\n",
    "\n",
    "    numOutputSpikes = len(outputSpikeTimes)\n",
    "    numOutputSpikesPerSim.append(numOutputSpikes)\n",
    "    listOfISIs += list(np.diff(outputSpikeTimes))\n",
    "    \n",
    "    listOfSingleSimulationDicts.append(currSimulationResultsDict)\n",
    "    \n",
    "    dataCollectionDurationInSeconds = (time.time() - collectionStartTime)\n",
    "    print(\"data collection per single simulation took %.4f seconds\" % (dataCollectionDurationInSeconds))\n",
    "    \n",
    "    entireSimulationDurationInMinutes = (time.time() - preparationStartTime) / 60\n",
    "    print('-----------------------------------------------------------')\n",
    "    print('finished simulation %d: num output spikes = %d' %(simInd + 1, numOutputSpikes))\n",
    "    print(\"entire simulation took %.2f minutes\" % (entireSimulationDurationInMinutes))\n",
    "    print('------------------------------------------------------------------------------/')\n",
    "\n",
    "    # increment simulation index\n",
    "    simInd = simInd + 1\n",
    "\n",
    "    # make sure we don't run forever\n",
    "    if simInd > 7 * numSimulations:\n",
    "        break\n",
    "\n",
    "##%% all simulations have ended\n",
    "exc_spikes_per_100ms_mean_range = list(np.array(exc_spikes_per_100ms_range_per_sim).mean(axis=0).astype(int))\n",
    "inh_spikes_per_100ms_mean_range = list(np.array(inh_spikes_per_100ms_range_per_sim).mean(axis=0).astype(int))\n",
    "totalNumOutputSpikes = sum(numOutputSpikesPerSim)\n",
    "totalNumSimulationSeconds = totalSimDurationInSec * numSimulations\n",
    "averageOutputFrequency = totalNumOutputSpikes / float(totalNumSimulationSeconds)\n",
    "ISICV = np.std(listOfISIs) / np.mean(listOfISIs)\n",
    "entireExperimentDurationInMinutes = (time.time() - experimentStartTime) / 60\n",
    "                            \n",
    "# calculate some collective meassures of the experiment\n",
    "print('...')\n",
    "print('...')\n",
    "print('...')\n",
    "print('-------------------------------------------------\\\\')\n",
    "print(\"entire experiment took %.2f minutes\" % (entireExperimentDurationInMinutes))\n",
    "print('-----------------------------------------------')\n",
    "print('total number of simulations is %d' %(len(numOutputSpikesPerSim)))\n",
    "print('total number of collected spikes is ' + str(totalNumOutputSpikes))\n",
    "print('average number of excitatory spikes per 100ms is: %s' %(str(exc_spikes_per_100ms_mean_range)))\n",
    "print('average number of inhibitory spikes per 100ms is: %s' %(str(inh_spikes_per_100ms_mean_range)))\n",
    "print('average output frequency is %.2f [Hz]' % (averageOutputFrequency))\n",
    "print('number of spikes per simulation minute is %.2f' % (totalNumOutputSpikes / entireExperimentDurationInMinutes))\n",
    "print('ISI-CV is ' + str(ISICV))\n",
    "print('-------------------------------------------------/')\n",
    "sys.stdout.flush()\n",
    "\n",
    "#%% organize and save everything\n",
    "\n",
    "# create a simulation parameters dict\n",
    "experimentParams = {}\n",
    "experimentParams['random_seed']    = random_seed\n",
    "experimentParams['numSimulations'] = numSimulations\n",
    "experimentParams['totalSimDurationInSec']  = totalSimDurationInSec\n",
    "experimentParams['morphology_description'] = morphology_description\n",
    "experimentParams['segments_to_keep'] = segments_to_keep\n",
    "experimentParams['segments_to_drop'] = segments_to_drop\n",
    "experimentParams['gmax_NMDA_to_AMPA_ratio'] = gmax_NMDA_to_AMPA_ratio\n",
    "experimentParams['Ih_vshift'] = Ih_vshift\n",
    "experimentParams['SKE2_mult_factor'] = SKE2_mult_factor\n",
    "experimentParams['keep_probability_below_01_output_spikes'] = keep_probability_below_01_output_spikes\n",
    "experimentParams['keep_probability_above_24_output_spikes'] = keep_probability_above_24_output_spikes\n",
    "experimentParams['max_output_spikes_to_keep_per_sim'] = max_output_spikes_to_keep_per_sim\n",
    "experimentParams['max_spikes_mult_factor_per_active_segment'] = max_spikes_mult_factor_per_active_segment\n",
    "experimentParams['max_spikes_mult_factor_per_NMDA_g_ratio'] = max_spikes_mult_factor_per_NMDA_g_ratio\n",
    "experimentParams['inh_max_delta_spikes_mult_factor_per_NMDA_g_ratio'] = inh_max_delta_spikes_mult_factor_per_NMDA_g_ratio\n",
    "experimentParams['exc_max_spikes_mult_factor']       = exc_max_spikes_mult_factor\n",
    "experimentParams['inh_max_delta_spikes_mult_factor'] = inh_max_delta_spikes_mult_factor\n",
    "\n",
    "experimentParams['numSamplesPerMS_HighRes'] = numSamplesPerMS_HighRes\n",
    "experimentParams['inst_rate_sampling_time_interval_options_ms'] = inst_rate_sampling_time_interval_options_ms\n",
    "experimentParams['temporal_inst_rate_smoothing_sigma_options_ms'] = temporal_inst_rate_smoothing_sigma_options_ms\n",
    "experimentParams['inst_rate_sampling_time_interval_jitter_range'] = inst_rate_sampling_time_interval_jitter_range\n",
    "experimentParams['temporal_inst_rate_smoothing_sigma_jitter_range'] = temporal_inst_rate_smoothing_sigma_jitter_range\n",
    "experimentParams['num_bas_ex_spikes_per_100ms_range']          = num_bas_ex_spikes_per_100ms_range\n",
    "experimentParams['num_bas_ex_inh_spike_diff_per_100ms_range']  = num_bas_ex_inh_spike_diff_per_100ms_range\n",
    "experimentParams['num_apic_ex_spikes_per_100ms_range']         = num_apic_ex_spikes_per_100ms_range\n",
    "experimentParams['num_apic_ex_inh_spike_diff_per_100ms_range'] = num_apic_ex_inh_spike_diff_per_100ms_range\n",
    "\n",
    "experimentParams['collectAndSaveDVTs']      = collectAndSaveDVTs\n",
    "experimentParams['allSectionsType']          = allSectionsType\n",
    "experimentParams['allSections_DistFromSoma'] = allSections_DistFromSoma\n",
    "experimentParams['allSectionsLength']        = allSectionsLength\n",
    "experimentParams['allSegmentsType']                 = allSegmentsType\n",
    "experimentParams['allSegmentsLength']               = allSegmentsLength\n",
    "experimentParams['allSegments_DistFromSoma']        = allSegments_DistFromSoma\n",
    "experimentParams['allSegments_SectionDistFromSoma'] = allSegments_SectionDistFromSoma\n",
    "experimentParams['allSegments_SectionInd']          = allSegments_SectionInd\n",
    "\n",
    "experimentParams['ISICV'] = ISICV\n",
    "experimentParams['listOfISIs'] = listOfISIs\n",
    "experimentParams['exc_spikes_per_100ms_range_per_sim'] = exc_spikes_per_100ms_range_per_sim\n",
    "experimentParams['inh_spikes_per_100ms_range_per_sim'] = inh_spikes_per_100ms_range_per_sim\n",
    "experimentParams['exc_spikes_per_100ms_mean_range'] = exc_spikes_per_100ms_mean_range\n",
    "experimentParams['inh_spikes_per_100ms_mean_range'] = inh_spikes_per_100ms_mean_range\n",
    "experimentParams['numOutputSpikesPerSim']     = numOutputSpikesPerSim\n",
    "experimentParams['totalNumOutputSpikes']      = totalNumOutputSpikes\n",
    "experimentParams['totalNumSimulationSeconds'] = totalNumSimulationSeconds\n",
    "experimentParams['averageOutputFrequency']    = averageOutputFrequency\n",
    "experimentParams['entireExperimentDurationInMinutes'] = entireExperimentDurationInMinutes\n",
    "\n",
    "# the important things to store\n",
    "experimentResults = {}\n",
    "experimentResults['listOfSingleSimulationDicts'] = listOfSingleSimulationDicts\n",
    "\n",
    "# the dict that will hold everything\n",
    "experimentDict = {}\n",
    "experimentDict['Params']  = experimentParams\n",
    "experimentDict['Results'] = experimentResults\n",
    "\n",
    "dirToSaveIn, filenameToSave = get_dir_name_and_filename(exc_spikes_per_100ms_mean_range, inh_spikes_per_100ms_mean_range, totalNumOutputSpikes, random_seed)\n",
    "if not os.path.exists(dirToSaveIn):\n",
    "    os.makedirs(dirToSaveIn)\n",
    "\n",
    "# pickle everythin\n",
    "pickle.dump(experimentDict, open(dirToSaveIn + filenameToSave, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
